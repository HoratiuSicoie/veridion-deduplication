# Veridion Product Deduplication

This project addresses the **deduplication of product records** that contain noise, inconsistencies, and redundancy. It includes:

- ðŸ“Š Exploratory Data Analysis (EDA)
- ðŸ§¹ Data cleaning
- ðŸ§  Local deduplication experiments using notebooks
- âš¡ Scalable deduplication with Apache Spark (in progress)

---

## ðŸ“¦ Installation

Set up your environment by installing the required dependencies:

```bash
pip install -r requirements.txt


VeridionDataEng/
â”œâ”€â”€ Data/
â”‚   â”œâ”€â”€ cleaned_products.parquet     # Cleaned product dataset
â”‚   â””â”€â”€ veridion_challange_data.parquet  # Initial Dataset
â”‚
â”œâ”€â”€ EDA/
â”‚   â”œâ”€â”€ eda.ipynb                    # EDA and data cleaning
â”‚   â””â”€â”€ deduplication_local.ipynb    # Local deduplication implementation
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ spark_session.py             # Spark session configuration
â”‚   â”œâ”€â”€ data_loader.py               # Data reading logic
â”‚   â”œâ”€â”€ deduplication.py             # TF-IDF and cosine similarity based matching
â”‚   â”œâ”€â”€ cluster_builder.py           # Clustering logic for assigning deduplication groups
â”‚   â””â”€â”€ consolidation.py             # Cluster-level consolidation
â”‚
â”œâ”€â”€ main.py                          # Entry point for Spark deduplication pipeline
â”œâ”€â”€ requirements.txt                 # Required Python packages
â””â”€â”€ README.md                        # Project documentation
```
---

## Notes
The implementation inside src/ is designed to scale to large datasets, but still requires further optimization:

Caching intermediate results between stages (especially after TF-IDF)

For now, the Spark implementation runs locally and may need tuning for performance on large datasets.

## ðŸ”§ Future Improvements
Add evaluation metrics for deduplication performance (precision, recall)

Additional similarity metrics (e.g. Jaccard, Levenshtein)

